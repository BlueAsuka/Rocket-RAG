{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HAOXUAN\\miniconda3\\envs\\agents2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from rocket_rag.utils import *\n",
    "from rocket_rag.node_indexing import *\n",
    "from rocket_rag.vector_store import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = '40kg'\n",
    "if_files_dict = parse_files(main_directory=INFERENCE_DIR)\n",
    "if_ts_files = if_files_dict[load]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random inference sample: ../data/inference/40kg\\spalling1\\spalling1_40_10_4.csv\n",
      "ROCKET features shape: (1, 20000)\n"
     ]
    }
   ],
   "source": [
    "# np.random.seed(42)\n",
    "\n",
    "rand_idx = np.random.randint(0, len(if_ts_files))\n",
    "if_ts_filename = if_ts_files[rand_idx]\n",
    "print(f'Random inference sample: {if_ts_filename}')\n",
    "if_rocket_feature = fit_transform([if_ts_filename],\n",
    "                                    field='current',\n",
    "                                    smooth=True,\n",
    "                                    smooth_ws=15,\n",
    "                                    tolist=False,\n",
    "                                    verbo=False)\n",
    "print(f'ROCKET features shape: {if_rocket_feature.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-05-29 17:13:09.363\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mrocket_rag.node_indexing\u001b[0m:\u001b[36mload_node_indexing\u001b[0m:\u001b[36m98\u001b[0m - \u001b[34m\u001b[1mLoading all nodes...\u001b[0m\n",
      "\u001b[32m2024-05-29 17:13:09.775\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrocket_rag.node_indexing\u001b[0m:\u001b[36mload_node_indexing\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mAll nodes are loaded.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "node_indexer = NodeIndexer()\n",
    "nodes = node_indexer.load_node_indexing(f'../store/nodes_{load}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = VectorStore()\n",
    "vector_store.add(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spalling1_40_8_5']\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "ids, dists = vector_store.ridge_query(if_rocket_feature)\n",
    "print(ids)\n",
    "print(dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the fault diagnosis reuslt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from prompts import fault_diagnosis_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_FILE = \"../config/configs.json\"\n",
    "with open(CONFIG_FILE) as f:\n",
    "    config = json.load(f)\n",
    "    GOOGLE_API_KEY = config[\"google_api_key\"]\n",
    "    GOOGLE_CSE_ID = config[\"google_cse_id\"]\n",
    "    OPENAI_API_KEY = config[\"openai_api_key\"]\n",
    "    GPT_MODEL = config[\"gpt_model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat with an intelligent assistant in your terminal\n",
    "from openai import OpenAI\n",
    "\n",
    "LOCAL = False\n",
    "\n",
    "# Point to the local server or use remote OpenAI GPT API\n",
    "local_llm_client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"not-needed\") \n",
    "openai_client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY', config[\"openai_api_key\"]))\n",
    "client = local_llm_client if LOCAL else openai_client\n",
    "model = \"local-model\" if LOCAL else GPT_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"fault_type\": \"spalling\",\n",
      "    \"degradation_level\": 1,\n",
      "    \"retrieval_result(s)\": \"spalling1_40_8_5\",\n",
      "    \"score\": 1.0,\n",
      "    \"distances\": [],\n",
      "    \"description\": \"The actuator is experiencing surface damage in the ball-screw mechanism at a low level, affecting its smoothness and efficiency. Immediate attention is recommended to prevent further deterioration and ensure optimal performance.\"\n",
      "}"
     ]
    }
   ],
   "source": [
    "history = [\n",
    "    {\"role\": \"system\", \"content\": fault_diagnosis_prompt.sys_prompt},\n",
    "    {\"role\": \"user\", \"content\": fault_diagnosis_prompt.ridge_prompt.format(res=str(ids), score=dists)},\n",
    "]\n",
    "\n",
    "completions = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=history,\n",
    "    response_format= { \"type\": \"json_object\" },\n",
    "    temperature=0.1,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "new_message = {\"role\": \"assistant\", \"content\": \"\"}\n",
    "\n",
    "for chunk in completions:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "        new_message[\"content\"] += chunk.choices[0].delta.content\n",
    "\n",
    "history.append(new_message)\n",
    "fault_diagnosis_res = new_message['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(json.loads(fault_diagnosis_res)))\n",
    "fault_diagnosis_json = json.loads(fault_diagnosis_res)\n",
    "fault_type = fault_diagnosis_json['fault_type']\n",
    "fault_description = fault_diagnosis_json['description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use multi-query generation for query parsing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import multi_queries_gen_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Missing required parameter: 'tools[0].type'.\", 'type': 'invalid_request_error', 'param': 'tools[0].type', 'code': 'missing_required_parameter'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m mq_messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      2\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: multi_queries_gen_prompt\u001b[38;5;241m.\u001b[39msys_prompt},\n\u001b[0;32m      3\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: multi_queries_gen_prompt\u001b[38;5;241m.\u001b[39muser_prompt\u001b[38;5;241m.\u001b[39mformat(fault_type\u001b[38;5;241m=\u001b[39mfault_type, \n\u001b[0;32m      4\u001b[0m                                                                             fault_description\u001b[38;5;241m=\u001b[39mfault_description, \n\u001b[0;32m      5\u001b[0m                                                                             num\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;241m5\u001b[39m))},\n\u001b[0;32m      6\u001b[0m ]\n\u001b[1;32m----> 8\u001b[0m completions \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# this field is currently unused\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmq_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m new_message \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m completions:\n",
      "File \u001b[1;32mc:\\Users\\HAOXUAN\\miniconda3\\envs\\agents2\\Lib\\site-packages\\openai\\_utils\\_utils.py:272\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    270\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HAOXUAN\\miniconda3\\envs\\agents2\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:645\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    643\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    644\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m--> 645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    651\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    654\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    657\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    660\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    661\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    662\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    663\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    664\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    665\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    666\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    667\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HAOXUAN\\miniconda3\\envs\\agents2\\Lib\\site-packages\\openai\\_base_client.py:1088\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1074\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1075\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1076\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1083\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1084\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1085\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1086\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1087\u001b[0m     )\n\u001b[1;32m-> 1088\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\HAOXUAN\\miniconda3\\envs\\agents2\\Lib\\site-packages\\openai\\_base_client.py:853\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    846\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    851\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    852\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HAOXUAN\\miniconda3\\envs\\agents2\\Lib\\site-packages\\openai\\_base_client.py:930\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[0;32m    928\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m--> 930\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m    933\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    934\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    937\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    938\u001b[0m )\n",
      "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Missing required parameter: 'tools[0].type'.\", 'type': 'invalid_request_error', 'param': 'tools[0].type', 'code': 'missing_required_parameter'}}"
     ]
    }
   ],
   "source": [
    "mq_messages = [\n",
    "    {\"role\": \"system\", \"content\": multi_queries_gen_prompt.sys_prompt},\n",
    "    {\"role\": \"user\", \"content\": multi_queries_gen_prompt.user_prompt.format(fault_type=fault_type, \n",
    "                                                                            fault_description=fault_description, \n",
    "                                                                            num=str(5))},\n",
    "]\n",
    "            \n",
    "completions = client.chat.completions.create(\n",
    "    model=model, # this field is currently unused\n",
    "    messages=mq_messages,\n",
    "    temperature=0.1,\n",
    "    tools=[{}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "new_message = {\"role\": \"assistant\", \"content\": \"\"}\n",
    "\n",
    "for chunk in completions:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "        new_message[\"content\"] += chunk.choices[0].delta.content\n",
    "\n",
    "history.append(new_message)\n",
    "multi_queries_gen = new_message['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def formalize_query(query: str):\n",
    "    \"\"\"Preprocess the query for the vector store query\n",
    "    \n",
    "    Remove some symbols including '-', '\"', '.' and indexing numbers or patterns like 1. 2. 3. ...\n",
    "    \"\"\"\n",
    "    query = query.strip().replace('\"', '').replace('. ', '')\n",
    "    pattern = re.compile(r'[-0-9]+|\\d+\\. ')\n",
    "    result = pattern.sub('', query)\n",
    "    return result.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['repair techniques for spalling in linear actuators',\n",
       " 'how to fix surface damage in ballscrew mechanisms of linear actuators',\n",
       " 'maintenance procedures for spalling in linear actuators',\n",
       " 'preventing further damage from spalling in linear actuators',\n",
       " 'best practices for addressing spalling in linear actuator systems']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_queries = [formalize_query(query) for query in multi_queries_gen.split('\\n')]\n",
    "generated_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use external tools for query search for decision-support\n",
    "Here use Google chrome web browser for a proof-of-concept validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from googleapiclient.discovery import build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"This includes reconditioning the actuator's ball screw, repairing and/or replacing worn or damaged internal components, replacing cover bands, and replacing all\\xa0...\", 'Jan 7, 2010 ... This service bulletin also gives instructions to remove/replace and repair the Horizontal Stabilizer Trim Actuator if a damaged. Ballscrew or\\xa0...', 'Often premature flaking or abnormal damage may lead to machine failure. Cause of the problem may include careless handling, excessive misalignment, insufficient\\xa0...', 'for Duff-Norton translating ball screw actuators. ... Do not allow actuator travel to go beyond cata- log closed height of actuator or serious damage to internal\\xa0...', 'Damage Condition, Possible Causes, Countermeasures. The raceways of the screw shaft and ball nut and/or the surface of the ball peel off like scales because\\xa0...', 'Check ball nut threads for damage and replace if necessary. 7. Check retaining wire location. Some are free to rotate to other part of nut when the return.', 'Troubleshooting - Damage by Type - Flaking occurs when small pieces of bearing material are split off from the smooth surface of the raceway or rolling\\xa0...', 'When time is of the essence, our emergency repair team will restore your damaged ... Spalling / Flaking Brinelling Vibration ... Thread-Craft, Inc. is a\\xa0...', 'inch screws, are cost effective and easy to install, maintain and repair. ... Check for metal fragments that may cause damage and could be an indication of broken\\xa0...', 'Structural failures such as excess of wear, cracking, backlash or spalling, malfunctions such as ball return channel jamming or seizure and lubricant\\xa0...']\n"
     ]
    }
   ],
   "source": [
    "def call_google(query: str, **kwargs):\n",
    "    \"\"\" Call the google chrome for searching online \"\"\"\n",
    "    \n",
    "    service = build(serviceName=\"customsearch\", \n",
    "                    version=\"v1\", \n",
    "                    developerKey=GOOGLE_API_KEY,\n",
    "                    static_discovery=False)\n",
    "    res = service.cse().list(q=query, cx=GOOGLE_CSE_ID, **kwargs).execute()\n",
    "    res_items = res[\"items\"]\n",
    "    res_snippets = [r['snippet'] for r in res_items]\n",
    "    return str(res_snippets)\n",
    "\n",
    "# A quick validation for the call_google\n",
    "print(call_google(query=generated_queries[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"call_google\",\n",
    "            \"description\": \"Call the google chrome web browser to search online based on a given query\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The query string for searching online\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"query\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_tools = {\"call_google\": call_google}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use ReAct Prompting to call the external functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rocket_rag.prompts import react_prompt\n",
    "# from colorama import Fore, Back, Style\n",
    "\n",
    "# # regular expression regex patterns\n",
    "# action_re = re.compile('^Action: (\\w+): (.*)$')\n",
    "# answer_re = re.compile(\"Answer: \")\n",
    "# answers = []\n",
    "\n",
    "# chrome_messages = [\n",
    "#     {\"role\": \"system\", \"content\": react_prompt.sys_prompt},\n",
    "#     {\"role\": \"user\", \"content\": react_prompt.user_prompt.format(query=generated_queries[0])},\n",
    "# ]\n",
    "\n",
    "# while True:\n",
    "#     response = client.chat.completions.create(\n",
    "#         model=model,\n",
    "#         messages=chrome_messages,\n",
    "#     )\n",
    "    \n",
    "#     # Get the response from the GPT and add it as a part of memory\n",
    "#     response_msg = response.choices[0].message.content\n",
    "#     history.append({\"role\": \"assistant\", \"content\": response_msg})\n",
    "    \n",
    "#     # If the respionse contains the keyword \"Answer: \", then return\n",
    "#     if answer_re.search(response_msg):\n",
    "#         print(Fore.YELLOW + response.choices[0].message.content)\n",
    "#         print(Style.RESET_ALL)\n",
    "#         answers.append(answer_re.search(response_msg).group(1))\n",
    "#         break\n",
    "    \n",
    "#     # Print the thinking process\n",
    "#     print(Fore.GREEN + response_msg)\n",
    "#     print(Style.RESET_ALL)\n",
    "\n",
    "#     # Take actions\n",
    "#     actions = [action_re.match(a) for a in response_msg.split(\"\\n\") if action_re.match(a)]\n",
    "#     if actions:\n",
    "#         action, action_input = actions[0].groups()\n",
    "#         try:\n",
    "#             print(Fore.CYAN + f\" -- running {action} {action_input}\")\n",
    "#             print(Style.RESET_ALL)\n",
    "#             # Apply available tools for the function execution\n",
    "#             obervation = available_tools[action](action_input) \n",
    "#             print(Fore.BLUE + f\"Observation: {obervation}\")\n",
    "#             print(Style.RESET_ALL)\n",
    "#             history.append({\"role\": \"user\", \"content\": \"Observation: \" + obervation})\n",
    "#         except:\n",
    "#             raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use GPT function calling to use external tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[32m2024-03-18 12:33:53.585\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m5\u001b[0m - \u001b[34m\u001b[1mProcessing 1/5 query...\u001b[0m\n",
      "\u001b[32m2024-03-18 12:33:55.613\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[34m\u001b[1m{'query': 'how to repair spalling damage in ballscrew actuators'}\u001b[0m\n",
      " 20%|██        | 1/5 [00:23<01:35, 23.87s/it]\u001b[32m2024-03-18 12:34:17.457\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m5\u001b[0m - \u001b[34m\u001b[1mProcessing 2/5 query...\u001b[0m\n",
      "\u001b[32m2024-03-18 12:34:19.458\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[34m\u001b[1m{'query': 'best practices for preventing spalling in linear actuators'}\u001b[0m\n",
      " 40%|████      | 2/5 [00:53<01:21, 27.12s/it]\u001b[32m2024-03-18 12:34:46.853\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m5\u001b[0m - \u001b[34m\u001b[1mProcessing 3/5 query...\u001b[0m\n",
      "\u001b[32m2024-03-18 12:34:49.054\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[34m\u001b[1m{'query': 'replacement options for ballscrew actuators with spalling damage'}\u001b[0m\n",
      " 60%|██████    | 3/5 [01:28<01:02, 31.01s/it]\u001b[32m2024-03-18 12:35:22.490\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m5\u001b[0m - \u001b[34m\u001b[1mProcessing 4/5 query...\u001b[0m\n",
      "\u001b[32m2024-03-18 12:35:24.804\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[34m\u001b[1m{'query': 'diagnosing spalling in linear actuators for effective maintenance'}\u001b[0m\n",
      " 80%|████████  | 4/5 [01:50<00:27, 27.30s/it]\u001b[32m2024-03-18 12:35:44.110\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m5\u001b[0m - \u001b[34m\u001b[1mProcessing 5/5 query...\u001b[0m\n",
      "\u001b[32m2024-03-18 12:35:46.089\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[34m\u001b[1m{'query': 'cost-effective solutions for spalling damage in actuator mechanisms'}\u001b[0m\n",
      "100%|██████████| 5/5 [02:15<00:00, 27.07s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "answers = []\n",
    "for i in tqdm(range(len(generated_queries))):\n",
    "    loguru.logger.debug(f'Processing {i+1}/{len(generated_queries)} query...')\n",
    "\n",
    "    gpt_tool_call_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful chatbot that can use web browser to offer reliable searching results to human beings.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Search for the following query using Google web browser: {generated_queries[i]}\"}\n",
    "    ]\n",
    "\n",
    "    first_response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=gpt_tool_call_messages,\n",
    "        tools=tools,\n",
    "    )\n",
    "    # print(first_response)\n",
    "    \n",
    "    gpt_tool_call_messages.append(first_response.choices[0].message)\n",
    "    tool_calls = first_response.choices[0].message.tool_calls\n",
    "    if tool_calls:\n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_to_call = available_tools[function_name]\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            loguru.logger.debug(function_args)\n",
    "            function_response = function_to_call(**function_args)\n",
    "\n",
    "            gpt_tool_call_messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": function_response,\n",
    "                }\n",
    "            )\n",
    "    \n",
    "    second_response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=gpt_tool_call_messages,\n",
    "        tools=tools,\n",
    "    )\n",
    "    # print(second_response)\n",
    "    \n",
    "    answers.append(second_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering all searching results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "from openai import AsyncClient\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "asyncclient = AsyncClient(api_key=OPENAI_API_KEY)\n",
    "\n",
    "async def async_combined_responses(query_responses: List[str], sys_prompt: str, num_children=3, debug=False):\n",
    "    \"\"\"Async version of combining responses from different nodes\"\"\"\n",
    "\n",
    "    node_batch_prompts = []\n",
    "    for idx in range(0, len(query_responses), num_children):\n",
    "        node_batch = query_responses[idx:idx+num_children]\n",
    "        node_batch_text = \"\\n\\n\".join([node for node in node_batch])\n",
    "\n",
    "        temp_prompt = f\"\"\" \\\n",
    "        Context information is below: \n",
    "        {node_batch_text}\n",
    "        Given the context information and not prior knowledge, summarize and present the result with detailed descriptions.\n",
    "        Combined result: \\\n",
    "        \"\"\"\n",
    "\n",
    "        node_batch_prompts.append(temp_prompt)\n",
    "    \n",
    "    tasks = [asyncclient.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": sys_prompt},\n",
    "            {\"role\": \"user\", \"content\": tp}\n",
    "        ]) for tp in node_batch_prompts\n",
    "    ]\n",
    "\n",
    "    combined_responses = await asyncio.gather(*tasks)\n",
    "    new_texts = [r.choices[0].message.content for r in combined_responses]\n",
    "\n",
    "    if len(new_texts) == 1:\n",
    "        loguru.logger.info(f\"Combined all responses to one. Done\")\n",
    "        return new_texts[0]\n",
    "    else:\n",
    "        loguru.logger.info(f\"Combined into {len(new_texts)} responses, keep combining\")\n",
    "        if debug:\n",
    "            loguru.logger.info(new_texts)\n",
    "        return await async_combined_responses(new_texts, sys_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-03-18 12:37:16.426\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36masync_combined_responses\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mCombined into 2 responses, keep combining\u001b[0m\n",
      "\u001b[32m2024-03-18 12:38:12.273\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36masync_combined_responses\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mCombined all responses to one. Done\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "combined_result_sys_prompt = f\"You are a helpful assistant that can summarize and extract useful information from give text as follow.\"\n",
    "combined_result = await async_combined_responses(answers, combined_result_sys_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save all contents into one markdown file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_expr = [str(i+1) + '. ' + generated_queries[i] + \"\\\\\" for i in range(len(generated_queries))]\n",
    "queries_expr = \"\\n\".join(queries_expr)\n",
    "anwsers_expr = ('\\n'.join(answers)).split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = \"*FAULT DIAGNOSIS REPORT\\n\\n\" + fault_diagnosis_res + \"\\n\\n\"\\\n",
    "\"Searching for: \\n\" + queries_expr + \"\\n\\n\"\n",
    "suggestions = \"Maintenance suggestions: \\n\" + combined_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = '../logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-03-18 12:38:27.811\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mSaved report to ../logs/fault-diagnosis2024-03-18-12-38-27.md\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dt = datetime.datetime.today().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "save_filename = 'fault-diagnosis-' + dt + '.md'\n",
    "save_filepath = os.path.join(LOG_DIR, save_filename)\n",
    "\n",
    "try:\n",
    "    with open(save_filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(report)\n",
    "        for i in range(len(anwsers_expr)):\n",
    "            f.write(f\"{anwsers_expr[i]} \\\\\")\n",
    "        f.write(suggestions)\n",
    "    loguru.logger.info(f\"Saved report to {save_filepath}\")\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Failed to save report to {save_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
