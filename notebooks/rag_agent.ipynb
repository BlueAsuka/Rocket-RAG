{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s381731\\AppData\\Local\\miniconda3\\envs\\agents\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from rocket_rag.utils import *\n",
    "from rocket_rag.node_indexing import *\n",
    "from rocket_rag.vector_store import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = '40kg'\n",
    "if_files_dict = parse_files(main_directory=INFERENCE_DIR)\n",
    "if_ts_files = if_files_dict[load]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random inference sample: ../data/inference/40kg\\lackLubrication2\\lackLubrication2_40_7_4.csv\n",
      "ROCKET features shape: (1, 20000)\n"
     ]
    }
   ],
   "source": [
    "# np.random.seed(42)\n",
    "\n",
    "rand_idx = np.random.randint(0, len(if_ts_files))\n",
    "if_ts_filename = if_ts_files[rand_idx]\n",
    "print(f'Random inference sample: {if_ts_filename}')\n",
    "if_rocket_feature = fit_transform([if_ts_filename],\n",
    "                                    field='current',\n",
    "                                    smooth=True,\n",
    "                                    smooth_ws=15,\n",
    "                                    tolist=False,\n",
    "                                    verbo=False)\n",
    "print(f'ROCKET features shape: {if_rocket_feature.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-03-17 14:21:01.258\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mrocket_rag.node_indexing\u001b[0m:\u001b[36mload_node_indexing\u001b[0m:\u001b[36m98\u001b[0m - \u001b[34m\u001b[1mLoading all nodes...\u001b[0m\n",
      "\u001b[32m2024-03-17 14:21:01.975\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrocket_rag.node_indexing\u001b[0m:\u001b[36mload_node_indexing\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mAll nodes are loaded.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "node_indexer = NodeIndexer()\n",
    "nodes = node_indexer.load_node_indexing(f'../store/nodes_{load}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = VectorStore()\n",
    "vector_store.add(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2.4026516730675422', '2.6995707092654024', '2.8018250691051385', '2.9966474927072353', '3.3901454900633676']\n",
      "['lackLubrication2_40_8_5', 'lackLubrication2_40_3_5', 'lackLubrication2_40_9_3', 'lackLubrication2_40_3_2', 'lackLubrication2_40_2_2']\n"
     ]
    }
   ],
   "source": [
    "s, ids = vector_store.knn_query(if_rocket_feature, k=5)\n",
    "print(s)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the fault diagnosis reuslt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rocket_rag.prompts import fault_diagnosis_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval results: ['lackLubrication2_40_8_5', 'lackLubrication2_40_3_5', 'lackLubrication2_40_9_3', 'lackLubrication2_40_3_2', 'lackLubrication2_40_2_2']\n",
      "Diagonsis results:\n",
      "Refined fault type1: Fault in lack of lubrication\n",
      "Inference evidence: ['lackLubrication2_40_8_5' with similarity score 3.39, 'lackLubrication2_40_3_5' with similarity score 2.69, 'lackLubrication2_40_9_3' with similarity score 2.8, 'lackLubrication2_40_3_2' with similarity score 2.99, 'lackLubrication2_40_2_2' with similarity score 2.4]\n",
      "Description of the Fault: The actuator is experiencing a lack of lubrication, which can lead to increased friction and potential wear and tear, impacting performance and longevity. This condition may worsen over time if not addressed promptly. It's recommended to schedule maintenance and ensure proper lubrication for the actuator to maintain optimal operation."
     ]
    }
   ],
   "source": [
    "# Chat with an intelligent assistant in your terminal\n",
    "from openai import OpenAI\n",
    "\n",
    "# Point to the local server\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"not-needed\")\n",
    "\n",
    "history = [\n",
    "    {\"role\": \"system\", \"content\": fault_diagnosis_prompt.sys_prompt},\n",
    "    {\"role\": \"user\", \"content\": fault_diagnosis_prompt.user_prompt.format(res=str(ids), score=s)},\n",
    "]\n",
    "\n",
    "completions = client.chat.completions.create(\n",
    "    model=\"local-model\", # this field is currently unused\n",
    "    messages=history,\n",
    "    temperature=0.1,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "new_message = {\"role\": \"assistant\", \"content\": \"\"}\n",
    "\n",
    "for chunk in completions:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "        new_message[\"content\"] += chunk.choices[0].delta.content\n",
    "\n",
    "history.append(new_message)\n",
    "fault_diagnosis_res = new_message['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use multi-query generation for query parsing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rocket_rag.prompts import multi_queries_gen_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. \"Linear actuator lack of lubrication maintenance: methods and best practices\"\n",
      "2. \"How often should I lubricate my linear actuator? Frequency and techniques\"\n",
      "3. \"Troubleshooting linear actuator: identifying and resolving lack of lubrication issues\"\n",
      "4. \"Linear actuator lubrication guide: types, tools, and procedures\"\n",
      "5. \"Maintaining optimal performance for linear actuators: addressing lack of lubrication concerns\""
     ]
    }
   ],
   "source": [
    "mq_messages = [\n",
    "    {\"role\": \"system\", \"content\": multi_queries_gen_prompt.sys_prompt},\n",
    "    {\"role\": \"user\", \"content\": multi_queries_gen_prompt.user_prompt.format(res=fault_diagnosis_res, num=5)},\n",
    "]\n",
    "            \n",
    "completions = client.chat.completions.create(\n",
    "    model=\"local-model\", # this field is currently unused\n",
    "    messages=mq_messages,\n",
    "    temperature=0.1,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "new_message = {\"role\": \"assistant\", \"content\": \"\"}\n",
    "\n",
    "for chunk in completions:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "        new_message[\"content\"] += chunk.choices[0].delta.content\n",
    "\n",
    "history.append(new_message)\n",
    "multi_queries_gen = new_message['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def formalize_query(query: str):\n",
    "    \"\"\"Preprocess the query for the vector store query\n",
    "    \n",
    "    Remove some symbols including '-' and indexing numbers or patterns like 1. 2. 3. ...\n",
    "    \"\"\"\n",
    "    query = query.strip().replace('\"', '').replace('. ', '')\n",
    "    pattern = re.compile(r'[-0-9]+|\\d+\\. ')\n",
    "    result = pattern.sub('', query)\n",
    "    return result.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Linear actuator lack of lubrication maintenance: methods and best practices',\n",
       " 'How often should I lubricate my linear actuator? Frequency and techniques',\n",
       " 'Troubleshooting linear actuator: identifying and resolving lack of lubrication issues',\n",
       " 'Linear actuator lubrication guide: types, tools, and procedures',\n",
       " 'Maintaining optimal performance for linear actuators: addressing lack of lubrication concerns']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_queries = [formalize_query(query) for query in multi_queries_gen.split('\\n')]\n",
    "generated_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use external tools for fault diagnosis maintenance support\n",
    "Here use Google chrome web browser for a proof-of-concept validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "CONFIG_FILE = \"../../config/config.json\"\n",
    "with open(CONFIG_FILE) as f:\n",
    "    config = json.load(f)\n",
    "    GOOGLE_API_KEY = config[\"google_api_key\"]\n",
    "    GOOGLE_CSE_ID = config[\"google_cse_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_google(query: str, **kwargs):\n",
    "    \"\"\" Call the google chrome for searching online \"\"\"\n",
    "    \n",
    "    service = build(serviceName=\"customsearch\", \n",
    "                    version=\"v1\", \n",
    "                    developerKey=GOOGLE_API_KEY,\n",
    "                    static_discovery=False)\n",
    "    res = service.cse().list(q=query, cx=GOOGLE_CSE_ID, **kwargs).execute()\n",
    "    res_items = res[\"items\"]\n",
    "    res_snippets = [r['snippet'] for r in res_items]\n",
    "    return str(res_snippets)\n",
    "\n",
    "# A quick validation for the call_google\n",
    "# print(call_google(query=generated_queries[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_functions = {\"call_google\": call_google}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rocket_rag.prompts import react_prompt\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "# regular expression regex patterns\n",
    "action_re = re.compile('^Action: (\\w+): (.*)$')\n",
    "answer_re = re.compile(\"Answer: \")\n",
    "answers = []\n",
    "\n",
    "chrome_messages = [\n",
    "    {\"role\": \"system\", \"content\": react_prompt.sys_prompt},\n",
    "    {\"role\": \"user\", \"content\": react_prompt.user_prompt.format(query=generated_queries[0])},\n",
    "]\n",
    "\n",
    "while True:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"local-model\",\n",
    "        messages=chrome_messages,\n",
    "    )\n",
    "    \n",
    "    # Get the response from the GPT and add it as a part of memory\n",
    "    response_msg = response.choices[0].message.content\n",
    "    history.append({\"role\": \"assistant\", \"content\": response_msg})\n",
    "    \n",
    "    # If the respionse contains the keyword \"Answer: \", then return\n",
    "    if answer_re.search(response_msg):\n",
    "        print(Fore.YELLOW + response.choices[0].message.content)\n",
    "        print(Style.RESET_ALL)\n",
    "        answers.append(response_msg)\n",
    "        break\n",
    "    \n",
    "    # Print the thinking process\n",
    "    print(Fore.GREEN + response_msg)\n",
    "    print(Style.RESET_ALL)\n",
    "\n",
    "    # Take actions\n",
    "    actions = [action_re.match(a) for a in response_msg.split(\"\\n\") if action_re.match(a)]\n",
    "    if actions:\n",
    "        action, action_input = actions[0].groups()\n",
    "        try:\n",
    "            print(Fore.CYAN + f\" -- running {action} {action_input}\")\n",
    "            print(Style.RESET_ALL)\n",
    "            # Apply available tools for the function execution\n",
    "            obervation = available_functions[action](action_input) \n",
    "            print(Fore.BLUE + f\"Observation: {obervation}\")\n",
    "            print(Style.RESET_ALL)\n",
    "            history.append({\"role\": \"user\", \"content\": \"Observation: \" + obervation})\n",
    "        except:\n",
    "            raise NotImplementedError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
